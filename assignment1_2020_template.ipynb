{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2020 Semester 1\n",
    "\n",
    "## Assignment 1: Naive Bayes Classifiers\n",
    "\n",
    "###### Submission deadline: 7 pm, Monday 20 Apr 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student Name(s):**    `PLEASE ENTER YOUR NAME(S) HERE`\n",
    "\n",
    "**Student ID(s):**     `PLEASE ENTER YOUR ID(S) HERE`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Student Name: Girish Madnani\n",
    "#Student ID: 934130"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you will use for your Assignment 1 submission.\n",
    "\n",
    "Marking will be applied on the four functions that are defined in this notebook, and to your responses to the questions at the end of this notebook (Submitted in a separate PDF file).\n",
    "\n",
    "**NOTE: YOU SHOULD ADD YOUR RESULTS, DIAGRAMS AND IMAGES FROM YOUR OBSERVATIONS IN THIS FILE TO YOUR REPORT (the PDF file).**\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find.\n",
    "\n",
    "**Adding proper comments to your code is MANDATORY. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1016,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should prepare the data by reading it from a file and converting it into a useful format for training and testing\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def preprocess(datasets):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    if (datasets == 'datasets/car.data'):\n",
    "        with open(datasets, mode='r') as fin:\n",
    "            for line in fin:\n",
    "                atts = line.strip().split(\",\")\n",
    "                X.append(atts[:-1]) #all atts, excluding the class\n",
    "                y.append(atts[-1])\n",
    "    \n",
    "        def convert_feature_1and2and6(raw):\n",
    "            if raw == \"low\": return 0\n",
    "            elif raw == \"med\": return 1\n",
    "            elif raw == \"high\": return 2\n",
    "            elif raw == \"vhigh\": return 3\n",
    "            # In general, we might want to catch unexpected values, too\n",
    "        def convert_feature_3(raw):\n",
    "            if raw == \"2\": return 0\n",
    "            elif raw == \"3\": return 1\n",
    "            elif raw == \"4\": return 2\n",
    "            elif raw == \"5more\": return 3\n",
    "        def convert_feature_4(raw):\n",
    "            if raw == \"2\": return 0\n",
    "            elif raw == \"4\": return 1\n",
    "            elif raw == \"more\": return 2\n",
    "        def convert_feature_5(raw):\n",
    "            if raw == \"small\": return 0\n",
    "            elif raw == \"med\": return 1\n",
    "            elif raw == \"big\": return 2\n",
    "        def convert_class(raw):\n",
    "            if raw == \"unacc\": return 0\n",
    "            elif raw == \"acc\": return 1\n",
    "            elif raw == \"good\": return 2\n",
    "            elif raw == \"vgood\": return 3\n",
    "\n",
    "        X_ordinal = []\n",
    "        for x in X:\n",
    "            f1, f2, f3, f4, f5, f6 = x\n",
    "            f1 = convert_feature_1and2and6(f1)\n",
    "            f2 = convert_feature_1and2and6(f2)\n",
    "            f3 = convert_feature_3(f3)\n",
    "            f4 = convert_feature_4(f4)\n",
    "            f5 = convert_feature_5(f5)\n",
    "            f6 = convert_feature_1and2and6(f6)\n",
    "            x = [f1, f2, f3, f4, f5, f6]\n",
    "            X_ordinal.append(x)\n",
    "\n",
    "        #convert to int array to make sure everything is converted.\n",
    "        X_ordinal = np.array(X_ordinal, dtype='int')\n",
    "\n",
    "\n",
    "        #convert ys\n",
    "        y_numeric = []\n",
    "        for this_y in y:\n",
    "            this_y = convert_class(this_y)\n",
    "            y_numeric.append(this_y)\n",
    "\n",
    "        y_num = np.array(y_numeric, dtype='int')\n",
    "\n",
    "\n",
    "\n",
    "    if (datasets == 'datasets/nursery.data'):\n",
    "        with open(datasets, mode='r') as fin:\n",
    "            for line in fin:\n",
    "                #print(line)\n",
    "                atts = line.strip().split(\",\")\n",
    "                X.append(atts[:-1]) #all atts, excluding the class\n",
    "                y.append(atts[-1])\n",
    "                \n",
    "        def convert_feature_1(raw):\n",
    "            if raw == \"usual\": return 0\n",
    "            elif raw == \"pretentious\": return 1\n",
    "            elif raw == \"great_pret\": return 2\n",
    "        def convert_feature_2(raw):\n",
    "            if raw == \"very_crit\": return 0\n",
    "            elif raw == \"critical\": return 1\n",
    "            elif raw == \"improper\": return 2\n",
    "            elif raw == \"less_proper\": return 3\n",
    "            elif raw == \"proper\": return 4\n",
    "        def convert_feature_3(raw):\n",
    "            if raw == \"foster\": return 0\n",
    "            elif raw == \"incomplete\": return 1\n",
    "            elif raw == \"complete\": return 2\n",
    "            elif raw == \"completed\": return 3\n",
    "        def convert_feature_4(raw):\n",
    "            if raw == \"1\": return 0\n",
    "            elif raw == \"2\": return 1\n",
    "            elif raw == \"3\": return 2\n",
    "            elif raw == \"more\": return 3\n",
    "        def convert_feature_5and6(raw):\n",
    "            if raw == \"critical\": return 0\n",
    "            elif raw == \"inconv\": return 1\n",
    "            elif raw == \"less_conv\": return 2\n",
    "            elif raw == \"convenient\": return 3\n",
    "        def convert_feature_7(raw):\n",
    "            if raw == \"problematic\": return 0\n",
    "            elif raw == \"slightly_prob\": return 1\n",
    "            elif raw == \"nonprob\": return 2\n",
    "        def convert_feature_8(raw):\n",
    "            if raw == \"not_recom\": return 0\n",
    "            elif raw == \"priority\": return 1\n",
    "            elif raw == \"recommended\": return 2\n",
    "        def convert_class(raw):\n",
    "            if raw == \"not_recom\": return 0\n",
    "            elif raw == \"recommend\": return 1\n",
    "            elif raw == \"very_recom\": return 2\n",
    "            elif raw == \"priority\": return 3\n",
    "            elif raw == \"spec_prior\": return 4\n",
    "\n",
    "\n",
    "        X_ordinal = []\n",
    "        for x in X:\n",
    "            fun1, fun2, fun3, fun4, fun5, fun6, fun7, fun8 = x\n",
    "            fun1 = convert_feature_1(fun1)\n",
    "            fun2 = convert_feature_2(fun2)\n",
    "            fun3 = convert_feature_3(fun3)\n",
    "            fun4 = convert_feature_4(fun4)\n",
    "            fun5 = convert_feature_5and6(fun5)\n",
    "            fun6 = convert_feature_5and6(fun6)\n",
    "            fun7 = convert_feature_7(fun7)\n",
    "            fun8 = convert_feature_8(fun8)\n",
    "            x = [fun1, fun2, fun3, fun4, fun5, fun6, fun7, fun8]\n",
    "            X_ordinal.append(x)\n",
    "\n",
    "        #convert to int array to make sure everything is converted.\n",
    "        X_ordinal = np.array(X_ordinal, dtype='int')\n",
    "\n",
    "\n",
    "        #convert ys\n",
    "        y_numeric = []\n",
    "        for this_y in y:\n",
    "            this_y = convert_class(this_y)\n",
    "            y_numeric.append(this_y)\n",
    "\n",
    "        y_num = np.array(y_numeric, dtype='int')\n",
    "\n",
    "\n",
    "    \n",
    "    if (datasets == 'datasets/somerville.data'):\n",
    "        with open(datasets, mode='r') as fin:\n",
    "            for line in fin:\n",
    "                atts = line.strip().split(\",\")\n",
    "                X.append(atts[1:7])\n",
    "                y.append(atts[0])\n",
    "        \n",
    "        X_ordinal = np.array(X, dtype='int')\n",
    "        y_num = np.array(y, dtype='int')\n",
    "\n",
    "   \n",
    "    if (datasets == 'datasets/wine.data'):\n",
    "        with open(datasets, mode='r') as fin:\n",
    "            for line in fin:\n",
    "                atts = line.strip().split(\",\")\n",
    "                X.append(atts[1:14]) #all atts, excluding the class\n",
    "                y.append(atts[0])\n",
    "        \n",
    "        X = np.array(X, dtype='float')\n",
    "        y = np.array(y, dtype='float')\n",
    "        \n",
    "        X_ordinal = X.astype(int)\n",
    "        y_num = y.astype(int)\n",
    "    \n",
    "    clf = LinearSVC()\n",
    "    clf.fit(X_ordinal, y_num)\n",
    "\n",
    "    return X_ordinal, y_num\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1017,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should calculat prior probabilities and likelihoods from the training data and using\n",
    "# them to build a naive Bayes model\n",
    "from collections import Counter\n",
    "from scipy.stats import norm\n",
    "def train(data):\n",
    "    \n",
    "    formated_values = preprocess(data)\n",
    "    Xy = [None]*len(set(formated_values[1]))\n",
    "    priors = [None]*len(set(formated_values[1]))\n",
    "    mean_std_XY = []\n",
    "    meanstd = []\n",
    "    posterior = []\n",
    "    post = []\n",
    "    final= []\n",
    "    likelihoods=[]\n",
    "    result = 1\n",
    "    res = []\n",
    "    dict_xy ={}\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range (len(set(formated_values[1]))):\n",
    "        Xy[i] = (formated_values[0])[formated_values[1]==i]\n",
    "        \n",
    "    for j in range (len(set(formated_values[1]))):\n",
    "        priors[j] = len(Xy[j]) / len(formated_values[1])\n",
    "        \n",
    "    for x in range (len(set(formated_values[1]))):\n",
    "        for z in range (formated_values[0].shape[1]):\n",
    "            mean_std_XY.append(mean_std((Xy[x])[:,z]))\n",
    "    \n",
    "    \n",
    "    for k in range (len(set(formated_values[1]))):\n",
    "        meanstd.append([])\n",
    "        for l in range ((formated_values[0].shape[1])):\n",
    "            meanstd[k].append(mean_std_XY[l])\n",
    "            if l == ((formated_values[0].shape[1])-1):\n",
    "                del mean_std_XY[0: (formated_values[0].shape[1] - 1)]\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    for e in range (len(set(formated_values[1]))):\n",
    "        posterior.append([])\n",
    "        for f in range (Xy[e].shape[0]):\n",
    "            posterior[e].append([])\n",
    "            for g in range (formated_values[0].shape[1]):\n",
    "                posterior[e][f].append(p_of_xy(Xy[e][f][g] ,meanstd[e][g][0],meanstd[e][g][1]))\n",
    "                \n",
    "     \n",
    "    \n",
    "    for t in range (len(posterior)):\n",
    "        post.append([])\n",
    "        for s in range (len(posterior[t])):\n",
    "            post[t].append(result)\n",
    "            result = 1\n",
    "            for r in range (len(posterior[t][s])):\n",
    "                result = result * posterior[t][s][r]\n",
    "                \n",
    "    \n",
    "    \n",
    "    for u in range (len(priors)):\n",
    "        final.append([])\n",
    "        for v in range (len(post[u])):\n",
    "            final[u].append(priors[u] * post[u][v])\n",
    "            \n",
    "#     print(\"priors:\", priors)\n",
    "#     print(\"likelihoods:\", final)\n",
    "    \n",
    "  \n",
    "    return Xy, final\n",
    "\n",
    "\n",
    "def mean_std(values):\n",
    "    mean = np.mean(values)\n",
    "    std = np.std(values)\n",
    "    return mean, std \n",
    "def p_of_xy(a, mean_xy, std_xy):\n",
    "    pxy = 1/(np.sqrt(2*np.pi)*std_xy) * np.exp((-a-mean_xy)**2/(2*(std_xy)**2))\n",
    "    return pxy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1018,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should predict classes for new items in a test dataset (for the purposes of this assignment, you\n",
    "# can re-use the training data as a test set)\n",
    "from collections import Counter\n",
    "\n",
    "def predict(data):\n",
    "    prob = train(data)\n",
    "    Xy = prob[0]\n",
    "    likelihoods = prob[1]\n",
    "    large=[]\n",
    "    indexes= []\n",
    "    attributes = []\n",
    "    row = []\n",
    "    score = 0\n",
    "    scores = []\n",
    "    predicted_list = []\n",
    "    \n",
    "    for a in range (len(likelihoods)):\n",
    "        b = max(likelihoods[a])\n",
    "        c = likelihoods[a].index(max(likelihoods[a]))\n",
    "        large.append(b)\n",
    "        indexes.append(c)\n",
    "        \n",
    "    \n",
    "    \n",
    "    for d in range (len(Xy)):\n",
    "        attributes.append(Xy[d][indexes[d]])\n",
    "           \n",
    "    row.append(attributes[large.index(max(large))])\n",
    "    \n",
    "    best_row = row[0]\n",
    "    \n",
    "    for i in range (len(Xy)):\n",
    "        for j in range (len(Xy[i])):\n",
    "            for k in range (len(Xy[i][j])):\n",
    "                if attributes[i][k] == Xy[i][j][k]:\n",
    "                    score += 1\n",
    "        score = score/len(attributes[i])\n",
    "        scores.append(score) \n",
    "                    \n",
    "                \n",
    "    mean_scores = sum(scores)/len(scores)\n",
    "    \n",
    "    for x in range (len(Xy)):\n",
    "        for y in range(len(Xy[x])):\n",
    "                if (np.equal(best_row,(Xy[x][y])).all()):\n",
    "                    predicted_list.append(x)\n",
    "\n",
    "    predicted = Counter(predicted_list)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"scores:\", scores)\n",
    "    print(\"mean scores:\", mean_scores)\n",
    "    print(\"predicted attributes:\", best_row)\n",
    "    print(\"predict:\" , predicted)\n",
    "    \n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1019,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'X_ordinal' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1019-13ed27f6c65a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datasets/______.data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1018-dfff8cd4ad35>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mXy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mlikelihoods\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1017-87584392de6d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mformated_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mXy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformated_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mpriors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformated_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1016-3295c3b372dd>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(datasets)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_ordinal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX_ordinal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_num\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'X_ordinal' referenced before assignment"
     ]
    }
   ],
   "source": [
    "predict('datasets/______.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should evaliate the prediction performance by comparing your model’s class outputs to ground\n",
    "# truth labels\n",
    "\n",
    "def evaluate():\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions \n",
    "\n",
    "\n",
    "If you are in a group of 1, you will respond to question (1), and **one** other of your choosing (two responses in total).\n",
    "\n",
    "If you are in a group of 2, you will respond to question (1) and question (2), and **two** others of your choosing (four responses in total). \n",
    "\n",
    "A response to a question should take about 100–250 words, and make reference to the data wherever possible.\n",
    "\n",
    "#### NOTE: you may develope codes or functions in respond to the question, but your formal answer should be added to a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1\n",
    "Try discretising the numeric attributes in these datasets and treating them as discrete variables in the na¨ıve Bayes classifier. You can use a discretisation method of your choice and group the numeric values into any number of levels (but around 3 to 5 levels would probably be a good starting point). Does discretizing the variables improve classification performance, compared to the Gaussian na¨ıve Bayes approach? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1.\tAs given, Data discretization is the process of converting continuous attribute values into  a finite set of numbers in order to loose less information. So the main reason for discretising  is to decrease the data fluctuation in order to “smoothen” out.\n",
    "2.\tDiscretising is done by creating a set of contiguous intervals to  transform a continuous valued variable into a discrete one. It is also done by dividing attributes into smaller segments which are then grouped by bins.\n",
    "3.\tEven though Gaussian naïve Bayes uses numeric attributes by default, various studies have shown that discretization of data has given better performance compared to Gaussian naïve Bayes \n",
    "4.\tThis is because discretization transforms the continuous variable values to discrete variable values and it acts as a variable selection method.\n",
    "5.\tDiscretization helps significantly in improving the classification performance of machine learning algorithms which are used for classification in high-dimensional data due to their robustness to the dimensionality of the data.\n",
    "6.\tDiscretization  also plays a significant role in improving algorithms  that are sensitive to the dimensionality of the data such as the  Naïve Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2\n",
    "Implement a baseline model (e.g., random or 0R) and compare the performance of the na¨ıve Bayes classifier to this baseline on multiple datasets. Discuss why the baseline performance varies across datasets, and to what extent the na¨ıve Bayes classifier improves on the baseline performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3\n",
    "Since it’s difficult to model the probabilities of ordinal data, ordinal attributes are often treated as either nominal variables or numeric variables. Compare these strategies on the ordinal datasets provided. Deterimine which approach gives higher classification accuracy and discuss why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4\n",
    "Evaluating the model on the same data that we use to train the model is considered to be a major mistake in Machine Learning. Implement a hold–out or cross–validation evaluation strategy (you should implement this yourself and do not simply call existing implementations from `scikit-learn`). How does your estimate of effectiveness change, compared to testing on the training data? Explain why. (The result might surprise you!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5\n",
    "Implement one of the advanced smoothing regimes (add-k, Good-Turing). Does changing the smoothing regime (or indeed, not smoothing at all) affect the effectiveness of the na¨ıve Bayes classifier? Explain why, or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6\n",
    "The Gaussian na¨ıve Bayes classifier assumes that numeric attributes come from a Gaussian distribution. Is this assumption always true for the numeric attributes in these datasets? Identify some cases where the Gaussian assumption is violated and describe any evidence (or lack thereof) that this has some effect on the NB classifier’s predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "By assuming the Gaussian Distribution, the Naïve Bayes can be stretched out to real-valued attributes. By doing this we form an extension of the Naïve Bayes which is called the Gaussian Naïve Bayes Classifier. Though other functions can be considered and used to project the distribution of the data. However, the gaussian distribution is used because it the easiest and simplest to apply as we only need to estimate and measure the mean and standard deviation from the given data.\n",
    "One problem with the gaussian distribution is that it only provides an estimation of the probability. \n",
    "Also, the gaussian distribution is only assumed if the input variables are real-values. But again, to assume that , it is necessary to remove any outliers for instance, the values that are more than three standard deviation from the mean because the algorithm will perform better only if the univariate distributions of our data are gaussian. \n",
    "So, Yes, the numeric attributes are treated as a continuous data and is then visualized as the actual data of the functions. However, as mentioned, the Gaussian assumptions can be violated by the outliers or if a class (Y) is very scattered. The model has to be linear or else it will be violated. The Class Y can be too heavily dependent on its attributes. \n",
    "Variance and standard deviation of the class maybe inconsistent. Some of the attributed are null or maybe random and can change. Some patterns may be difficult and hard to read and understand."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
